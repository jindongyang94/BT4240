{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Experimentation\n",
    "First, we import all relevant packages\n",
    "\n",
    "The crossvalidation's train_test_split() help us by splitting data into train & test set. This is easy way out before we do further processing:\n",
    "We should preprocess the data by partioning with the same percentage for training, cross_validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data before feature selection\n",
    "input_data_before_fs = pd.read_csv('processed_train.csv', index_col=0)\n",
    "\n",
    "# Input data after feature selection\n",
    "input_data_after_fs = pd.read_csv('processed_train_after_feature.csv', index_col=0)\n",
    "\n",
    "# Upsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "# Downsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "\n",
    "# List of all the input data\n",
    "input_all = {\n",
    "    \"normal_before_fs\" : input_data_before_fs,\n",
    "#     \"normal_after_fs\" : input_data_after_fs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input in all_input:\n",
    "#     print (\"Dataset Length:: \", len(input))\n",
    "#     print (\"Dataset Shape: \", input.shape)\n",
    "#     input_data.info()\n",
    "#     input_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    #Split data into variables types - boolean, categorical, continuous, ID\n",
    "    bool_var = list(data.select_dtypes(['bool']))\n",
    "    cont_var = list(data.select_dtypes(['float64']))\n",
    "    cat_var = list(data.select_dtypes(['int64']))\n",
    "\n",
    "    #Input Data can be from all except id details\n",
    "    final_input_data = data[cat_var + cont_var + bool_var]\n",
    "    \n",
    "    x = final_input_data.loc[:, final_input_data.columns != 'Target'].values\n",
    "    y = final_input_data['Target'].values\n",
    "    y=y.astype('int')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, \n",
    "                                                    random_state = 100 , stratify = y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def tenfold(model, x, y, metric='accuracy'):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "    scores = cross_validate(model, x, y, cv=kfold, scoring=metric, \n",
    "                            return_train_score=True)\n",
    "    return scores\n",
    "\n",
    "# accuracy_mean = scores['test_score'].mean()\n",
    "# accuracy_std = scores['train_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "def metric_consolidation(input_all, classifier, method = \"cross_validation\"):\n",
    "    metrics = {'accuracy': 'accuracy',\n",
    "               'roc_auc': make_scorer(multiclass_roc_auc_score, average='weighted'),\n",
    "               'f1_weighted': 'f1_weighted'\n",
    "              }\n",
    "    \n",
    "    for input_name, input_data in input_all.items():\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        # apply all metrics to the classifier for cross_validation\n",
    "        if method == \"cross_validation\":\n",
    "            scores = tenfold(classifier, x_train, y_train, metric = metrics)\n",
    "            print (\"Metrics for %s: \\n\" %input_name)\n",
    "            for metric in metrics:\n",
    "                test_score_name = \"test_\" + metric\n",
    "                test_score = scores[test_score_name]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100))   \n",
    "            print (\"\\n\")\n",
    "            \n",
    "        if method == \"test\":\n",
    "            y_pred = classifier.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            roc_score = multiclass_roc_auc_score(y_test, y_pred, average='weighted')\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            metric_values = {'accuracy': accuracy,\n",
    "                             'roc_auc': roc_score,\n",
    "                             'f1_weighted': f1_weighted\n",
    "                            }\n",
    "            for metric in metrics:\n",
    "                test_score = metric_values[metric]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 2 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 58.82 +/- 3.26\n",
      "f1_weighted Test Score: 58.78 +/- 2.69\n",
      "roc_auc Test Score: 63.61 +/- 2.11\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 2 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.64 +/- 1.80\n",
      "f1_weighted Test Score: 59.25 +/- 1.84\n",
      "roc_auc Test Score: 60.15 +/- 2.03\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 2 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.98 +/- 1.75\n",
      "f1_weighted Test Score: 58.63 +/- 1.87\n",
      "roc_auc Test Score: 59.70 +/- 3.32\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 50 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 64.20 +/- 1.80\n",
      "f1_weighted Test Score: 61.40 +/- 2.90\n",
      "roc_auc Test Score: 64.06 +/- 3.50\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 50 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.64 +/- 1.80\n",
      "f1_weighted Test Score: 59.25 +/- 1.84\n",
      "roc_auc Test Score: 60.15 +/- 2.03\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 50 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.98 +/- 1.75\n",
      "f1_weighted Test Score: 58.63 +/- 1.87\n",
      "roc_auc Test Score: 59.70 +/- 3.32\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 100 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 64.97 +/- 1.31\n",
      "f1_weighted Test Score: 60.10 +/- 1.52\n",
      "roc_auc Test Score: 61.91 +/- 2.32\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 100 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.64 +/- 1.80\n",
      "f1_weighted Test Score: 59.25 +/- 1.84\n",
      "roc_auc Test Score: 60.15 +/- 2.03\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: gini \n",
      " min split: 100 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.98 +/- 1.75\n",
      "f1_weighted Test Score: 58.63 +/- 1.87\n",
      "roc_auc Test Score: 59.70 +/- 3.32\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 2 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 58.15 +/- 2.50\n",
      "f1_weighted Test Score: 58.20 +/- 2.05\n",
      "roc_auc Test Score: 63.16 +/- 1.83\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 2 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.28 +/- 2.19\n",
      "f1_weighted Test Score: 58.58 +/- 2.43\n",
      "roc_auc Test Score: 59.81 +/- 2.38\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 2 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.99 +/- 1.29\n",
      "f1_weighted Test Score: 57.69 +/- 1.14\n",
      "roc_auc Test Score: 58.17 +/- 2.24\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 50 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 64.25 +/- 1.44\n",
      "f1_weighted Test Score: 60.59 +/- 1.07\n",
      "roc_auc Test Score: 62.92 +/- 1.78\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 50 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.28 +/- 2.19\n",
      "f1_weighted Test Score: 58.58 +/- 2.43\n",
      "roc_auc Test Score: 59.81 +/- 2.38\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 50 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.99 +/- 1.29\n",
      "f1_weighted Test Score: 57.69 +/- 1.14\n",
      "roc_auc Test Score: 58.17 +/- 2.24\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 100 \n",
      " min leaf: 1 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.32 +/- 1.59\n",
      "f1_weighted Test Score: 59.98 +/- 1.98\n",
      "roc_auc Test Score: 61.64 +/- 2.90\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 100 \n",
      " min leaf: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.28 +/- 2.19\n",
      "f1_weighted Test Score: 58.58 +/- 2.43\n",
      "roc_auc Test Score: 59.81 +/- 2.38\n",
      "\n",
      "\n",
      "For decision tree with: \n",
      " criterion: entropy \n",
      " min split: 100 \n",
      " min leaf: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.99 +/- 1.29\n",
      "f1_weighted Test Score: 57.69 +/- 1.14\n",
      "roc_auc Test Score: 58.17 +/- 2.24\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Search through parameters to fill up the values\n",
    "min_sample_split_values = [2, 50, 100]\n",
    "min_sample_leaf_values = [1, 50, 100]\n",
    "criterion_values = ['gini', 'entropy']\n",
    "\n",
    "for criterion in criterion_values:\n",
    "    for min_sample_split in min_sample_split_values:\n",
    "        for min_sample_leaf in min_sample_leaf_values:\n",
    "            decision_tree = DecisionTreeClassifier(class_weight=None, criterion=criterion, max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=min_sample_leaf,\n",
    "                                         min_samples_split=min_sample_split, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "            \n",
    "            print (\"For decision tree with: \\n criterion: %s \\n min split: %s \\n min leaf: %s \\n\"\n",
    "                  %(criterion, min_sample_split, min_sample_leaf))\n",
    "            metric_consolidation(input_all, decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Test Score: 61.16 +/- 0.00\n",
      "roc_auc Test Score: 60.69 +/- 0.00\n",
      "f1_weighted Test Score: 58.39 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Test Values for Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "\n",
    "metric_consolidation(input_all, decision_tree,method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphing_decisiontree(input_data, criterion='entropy'):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "    \n",
    "    ### Max Depth ------------------------------------------------------------------------------------\n",
    "    max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for max_depth in max_depths:\n",
    "        dt = DecisionTreeClassifier(class_weight=None, criterion=criterion, max_depth=max_depth,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "        scores = tenfold(dt, x_train, y_train)\n",
    "        accuracy_train = scores['train_score'].mean()\n",
    "        # Add acc score to previous train results\n",
    "        train_results.append(accuracy_train)\n",
    "        accuracy_test = scores['test_score'].mean()\n",
    "        # Add acc score to previous test results\n",
    "        test_results.append(accuracy_test)\n",
    "\n",
    "    line1, = plt.plot(max_depths, train_results,'b', label=\"Average CV Train Accuracy\")\n",
    "    line2, = plt.plot(max_depths, test_results, 'r', label=\"Average CV Test Accuracy\")\n",
    "    plt.legend(handler_map={line2: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel(\"Average Accuracy score\")\n",
    "    plt.xlabel(\"Tree depth\")\n",
    "    plt.show()\n",
    "\n",
    "    # Finding the best score and parameter to use\n",
    "    best_accuracy_score = max(test_results)\n",
    "    best_max_depth = max_depths[test_results.index(best_accuracy_score)]\n",
    "    print ('Best Max Depth Value:', best_max_depth)\n",
    "    print ('Corresponding Accuracy Value:', best_accuracy_score)\n",
    "    \n",
    "    ### Min Sample Splits ------------------------------------------------------------------------------------\n",
    "    min_samples_splits = np.linspace(2, 100, 99, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for min_samples_split in min_samples_splits:\n",
    "        dt = DecisionTreeClassifier(class_weight=None, criterion=criterion, max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=int(min_samples_split), min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "        scores = tenfold(dt, x_train, y_train)\n",
    "        accuracy_train = scores['train_score'].mean()\n",
    "        # Add acc score to previous train results\n",
    "        train_results.append(accuracy_train)\n",
    "        accuracy_test = scores['test_score'].mean()\n",
    "        # Add acc score to previous test results\n",
    "        test_results.append(accuracy_test)\n",
    "\n",
    "    line1, = plt.plot(min_samples_splits, train_results,'b', label=\"Average CV Train Accuracy\")\n",
    "    line2, = plt.plot(min_samples_splits, test_results, 'r', label=\"Average CV Test Accuracy\")\n",
    "    plt.legend(handler_map={line2: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel(\"Average Accuracy score\")\n",
    "    plt.xlabel(\"min samples split\")\n",
    "    plt.show()\n",
    "\n",
    "    # Finding the best score and parameter to use\n",
    "    best_accuracy_score = max(test_results)\n",
    "    best_min_samples_split = min_samples_splits[test_results.index(best_accuracy_score)]\n",
    "    print ('Best Min Sample Split Value:', best_min_samples_split)\n",
    "    print ('Corresponding Accuracy Value:', best_accuracy_score)\n",
    "    \n",
    "    ### Min Samples Leaf ------------------------------------------------------------------------------------\n",
    "    min_samples_leafs = np.linspace(1, 80, 80, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for min_samples_leaf in min_samples_leafs:\n",
    "        dt = DecisionTreeClassifier(class_weight=None, criterion=criterion, max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=int(min_samples_leaf),\n",
    "                                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "        scores = tenfold(dt, x_train, y_train)\n",
    "        accuracy_train = scores['train_score'].mean()\n",
    "        # Add acc score to previous train results\n",
    "        train_results.append(accuracy_train)\n",
    "        accuracy_test = scores['test_score'].mean()\n",
    "        # Add acc score to previous test results\n",
    "        test_results.append(accuracy_test)\n",
    "\n",
    "    line1, = plt.plot(min_samples_leafs, train_results,'b', label=\"Average CV Train Accuracy\")\n",
    "    line2, = plt.plot(min_samples_leafs, test_results, 'r', label=\"Average CV Test Accuracy\")\n",
    "    plt.legend(handler_map={line2: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel(\"Average Accuracy score\")\n",
    "    plt.xlabel(\"min samples leaf\")\n",
    "    plt.show()\n",
    "\n",
    "    # Finding the best score and parameter to use\n",
    "    best_accuracy_score = max(test_results)\n",
    "    best_min_samples_leaf = min_samples_leafs[test_results.index(best_accuracy_score)]\n",
    "    print ('Best Min Samples Leaf Value:', best_min_samples_leaf)\n",
    "    print ('Corresponding Accuracy Value:', best_accuracy_score)\n",
    "    \n",
    "    ### Max Features ------------------------------------------------------------------------------------\n",
    "    max_features = list(range(1,input_data.shape[1]))\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for max_feature in max_features:\n",
    "        dt = DecisionTreeClassifier(class_weight=None, criterion=criterion, max_depth=None,\n",
    "                                         max_features=max_feature, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "        scores = tenfold(dt, x_train, y_train)\n",
    "        accuracy_train = scores['train_score'].mean()\n",
    "        # Add acc score to previous train results\n",
    "        train_results.append(accuracy_train)\n",
    "        accuracy_test = scores['test_score'].mean()\n",
    "        # Add acc score to previous test results\n",
    "        test_results.append(accuracy_test)\n",
    "\n",
    "    line1, = plt.plot(max_features, train_results,'b', label=\"Average CV Train Accuracy\")\n",
    "    line2, = plt.plot(max_features, test_results, 'r', label=\"Average CV Test Accuracy\")\n",
    "    plt.legend(handler_map={line2: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel(\"Average Accuracy score\")\n",
    "    plt.xlabel(\"max_features\")\n",
    "    plt.show()\n",
    "\n",
    "    # Finding the best score and parameter to use\n",
    "    best_accuracy_score = max(test_results)\n",
    "    best_max_feature = max_features[test_results.index(best_accuracy_score)]\n",
    "    print ('Best Max Feature Value:', best_max_feature)\n",
    "    print ('Corresponding Accuracy Value:', best_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini\n",
    "clf_gini = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                     max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                     presort=False, random_state=100, splitter='best')\n",
    "#Entropy\n",
    "clf_entropy = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
    "                                     max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                     presort=False, random_state=100, splitter='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metric For Dataset before Feature Selection\n",
    "1. Accuracy Score\n",
    "2. F1 Score\n",
    "3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using Dataset before Feature Selection\n",
    "Y_predict_entropy_initial = clf_entropy.predict(x_test)\n",
    "Y_predict_gini_initial = clf_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Accuracy Score for Dataset before Feature Selection\n",
    "\n",
    "print ('Testing acc for entropy before feature selection is %f' %accuracy_score(Y_predict_entropy_initial, Y_test))\n",
    "print ('Testing acc for gini before feature selection is %f' %accuracy_score(Y_predict_gini_initial, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on F1 Score for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing f1 score for entropy before feature selection is %f' %f1_score(Y_test, Y_predict_entropy_initial, labels=[1,2,3,4], average='weighted'))\n",
    "print ('Testing f1 score for gini before feature selection is %f' %f1_score(Y_test, Y_predict_gini_initial, labels=[1,2,3,4], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Confusion Matrix for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing confusion matrix for entropy before feature selection is \\n', confusion_matrix(Y_test, Y_predict_entropy_initial, labels=[1,2,3,4]))\n",
    "print ('Testing confusion matrix for gini before feature selection is \\n', confusion_matrix(Y_test, Y_predict_gini_initial, labels=[1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'f1_score', 'confusion_matrix']\n",
    "\n",
    "for input_data in input_all:\n",
    "    x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric == \"accuracy\":\n",
    "            print ('Testing acc for entropy before feature selection is %f' %accuracy_score(Y_predict_entropy_initial, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metric For Dataset after Feature Selection\n",
    "1. Accuracy Score\n",
    "2. F1 Score\n",
    "3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using Dataset after Feature Selection\n",
    "Y_predict_entropy_initial_fs = clf_entropy_fs.predict(X_test_fs)\n",
    "Y_predict_gini_initial_fs = clf_gini_fs.predict(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Accuracy Score for Dataset after Feature Selection\n",
    "\n",
    "# print ('Testing acc for entropy after feature selection is %f' %accuracy_score(Y_test_fs, Y_predict_entropy_initial_fs))\n",
    "# print ('Testing acc for gini after feature selection is %f' %accuracy_score(Y_predict_gini_initial_fs, Y_test_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_entropy_results = tenfold(clf_entropy_fs, X, Y)\n",
    "print ('Testing acc for entropy after feature selection is %f. Std dv is (+/-) %f.' %(cv_entropy_results['test_score'].mean(), cv_entropy_results['test_score'].std()*2))\n",
    "\n",
    "cv_gini_results = tenfold(clf_gini_fs, X, Y)\n",
    "print ('Testing acc for gini after feature selection is %f. Std dv is (+/-) %f.' %(cv_gini_results['test_score'].mean(), cv_gini_results['test_score'].std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on F1 Score for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing f1 score for entropy after feature selection is %f' %f1_score(Y_test_fs, Y_predict_entropy_initial_fs, labels=[1,2,3,4], average='weighted'))\n",
    "print ('Testing f1 score for gini after feature selection is %f' %f1_score(Y_test_fs, Y_predict_gini_initial_fs, labels=[1,2,3,4], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Confusion Matrix for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing confusion matrix for entropy after feature selection is \\n', confusion_matrix(Y_test_fs, Y_predict_entropy_initial_fs, labels=[1,2,3,4]))\n",
    "print ('Testing confusion matrix for gini after feature selection is \\n', confusion_matrix(Y_test_fs, Y_predict_gini_initial_fs, labels=[1,2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at classification report for the initial modelling.\n",
    "print (\"Classification report for Gini: \\n\", classification_report(Y_test, Y_predict_gini_initial))\n",
    "print (\"Classification report for Entropy: \\n\", classification_report(Y_test, Y_predict_entropy_initial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Parameters\n",
    "\n",
    "We are going to plot each parameters on a graph, based on accuracy score as the performance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_all['normal_before_fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing_decisiontree(input_data, criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing_decisiontree(input_data, criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Parameters (Combining Parameters)\n",
    "\n",
    "We are going to plot each parameters on a graph, based on accuracy score as the performance metric. Optimize one feature after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function for this instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final Tuning of Parameters\n",
    "Lets try combining them together first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy_final = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3.0,\n",
    "                                     max_features=30, max_leaf_nodes=None,\n",
    "                                     min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "                                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                     presort=False, random_state=100, splitter='best')\n",
    "clf_entropy_final.fit(X_train_fs, Y_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict( ) will do the model prediction, predict y based on the input x\n",
    "Y_predict_entropy_final = clf_entropy_final.predict(X_test)\n",
    "print ('testing acc for entropy is %f' %accuracy_score(Y_predict_entropy_final, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try the other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini_final = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                     max_features=None, max_leaf_nodes=None,\n",
    "                                     min_impurity_split=1e-07, min_samples_leaf=50,\n",
    "                                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                     presort=False, random_state=100, splitter='best')\n",
    "clf_gini_final.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict( ) will do the model prediction, predict y based on the input x\n",
    "Y_predict_gini_final = clf_gini_final.predict(X_test)\n",
    "print ('testing acc for gini is %f' %accuracy_score(Y_predict_gini_final, Y_test_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at classification report for the above tuning.\n",
    "print (\"Classification report for Gini: \\n\", classification_report(Y_test_fs, Y_predict_gini_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Confusion Matrix for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing confusion matrix for gini after feature selection is \\n', confusion_matrix(Y_test_fs, Y_predict_gini_final, labels=[1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at classification report for the above tuning.\n",
    "print (\"Classification report for Entropy: \\n\", classification_report(Y_test_fs, Y_predict_entropy_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess predict score based on Confusion Matrix for Dataset after Feature Selection\n",
    "\n",
    "print ('Testing confusion matrix for entropy after feature selection is \\n', confusion_matrix(Y_test_fs, Y_predict_entropy_final, labels=[1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf_gini_final, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf_entropy_final, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
