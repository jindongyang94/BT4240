{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "The hype. So I wanna join in and try the hype\n",
    "\n",
    "## VotingClassifier (Soft and Hard)\n",
    "It is the ultimate classifier for classical machine learning methods so why not try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data before feature selection\n",
    "input_data_before_fs = pd.read_csv('processed_train.csv', index_col=0)\n",
    "\n",
    "# Input data after feature selection\n",
    "input_data_after_fs = pd.read_csv('processed_train_after_feature.csv', index_col=0)\n",
    "\n",
    "# Upsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "# Downsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "\n",
    "# List of all the input data\n",
    "input_all = {\n",
    "#     \"normal_before_fs\" : input_data_before_fs,\n",
    "    \"normal_after_fs\" : input_data_after_fs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "def metric_consolidation(input_all, classifier, method = \"cross_validation\"):\n",
    "    metrics = {'accuracy': 'accuracy',\n",
    "               'roc_auc': make_scorer(multiclass_roc_auc_score, average='weighted'),\n",
    "               'f1_weighted': 'f1_weighted'\n",
    "              }\n",
    "    \n",
    "    for input_name, input_data in input_all.items():\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        # apply all metrics to the classifier for cross_validation\n",
    "        if method == \"cross_validation\":\n",
    "            scores = tenfold(classifier, x_train, y_train, metric = metrics)\n",
    "            print (\"Metrics for %s: \\n\" %input_name)\n",
    "            for metric in metrics:\n",
    "                test_score_name = \"test_\" + metric\n",
    "                test_score = scores[test_score_name]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100))   \n",
    "            print (\"\\n\")\n",
    "            \n",
    "        if method == \"test\":\n",
    "            y_pred = classifier.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            roc_score = multiclass_roc_auc_score(y_test, y_pred, average='weighted')\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "            \n",
    "            metric_values = {'accuracy': accuracy,\n",
    "                             'roc_auc': roc_score,\n",
    "                             'f1_weighted': f1_weighted\n",
    "                            }\n",
    "            for metric in metrics:\n",
    "                test_score = metric_values[metric]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100)) \n",
    "            \n",
    "def preprocessing(data):\n",
    "    #Split data into variables types - boolean, categorical, continuous, ID\n",
    "    bool_var = list(data.select_dtypes(['bool']))\n",
    "    cont_var = list(data.select_dtypes(['float64']))\n",
    "    cat_var = list(data.select_dtypes(['int64']))\n",
    "\n",
    "    #Input Data can be from all except id details\n",
    "    final_input_data = data[cat_var + cont_var + bool_var]\n",
    "    \n",
    "    x = final_input_data.loc[:, final_input_data.columns != 'Target'].values\n",
    "    y = final_input_data['Target'].values\n",
    "    y=y.astype('int')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, \n",
    "                                                    random_state = 100 , stratify = y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def tenfold(model, x, y, metric='accuracy'):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "    scores = cross_validate(model, x, y, cv=kfold, scoring=metric, \n",
    "                            return_train_score=True)\n",
    "    return scores\n",
    "\n",
    "# accuracy_mean = scores['test_score'].mean()\n",
    "# accuracy_std = scores['train_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9  12   0  34]\n",
      " [  6  31   6  68]\n",
      " [  3  15   8  63]\n",
      " [  4  27   3 455]]\n",
      "accuracy Test Score: 67.61 +/- 0.00\n",
      "f1_weighted Test Score: 62.19 +/- 0.00\n",
      "roc_auc Test Score: 61.75 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()\n",
    "metric_consolidation(input_all, clf, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = svm.SVC(C=0.1,decision_function_shape='ovo', kernel='linear', max_iter=-1, random_state=100)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "clf3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "clf4 = LogisticRegression(random_state=100, penalty = 'l1', C = 10**-1)\n",
    "clf5 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                         random_state=0)\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svm', clf1), ('rf', clf2), ('dt', clf3), ('lr', clf4), \n",
    "                                    ('bgt', clf5)], \n",
    "                        voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Voting Classifier with: \n",
      " voting: hard \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.73 +/- 2.44\n",
      "f1_weighted Test Score: 62.85 +/- 3.03\n",
      "roc_auc Test Score: 63.94 +/- 2.43\n",
      "\n",
      "\n",
      "For Voting Classifier with: \n",
      " voting: soft \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.93 +/- 2.29\n",
      "f1_weighted Test Score: 61.68 +/- 2.80\n",
      "roc_auc Test Score: 62.69 +/- 2.13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "voting_values = ['hard', 'soft']\n",
    "\n",
    "for voting in voting_values:\n",
    "#     svm = svm.SVC(C=0.1,decision_function_shape='ovo', kernel='linear', \n",
    "#                    max_iter=-1, random_state=100)\n",
    "    dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                             max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                             min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
    "                                             presort=False, random_state=100, splitter='best')\n",
    "    lr = LogisticRegression(random_state=100, penalty = 'l1', C = 10**-1)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "    gbt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,random_state=0)\n",
    "    bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                            random_state=100)\n",
    "    \n",
    "    \n",
    "    eclf = VotingClassifier(estimators=[('rf', rf), ('dt', dt) , ('lr', lr), \n",
    "                                        ('gdt', gdt), ('bag', bag)], voting=voting)\n",
    "#     eclf = VotingClassifier(estimators=[('rf', rf), ('bgt', clf5), ('bag', clf6)], \n",
    "#                             voting=voting)\n",
    "    \n",
    "    print (\"For Voting Classifier with: \\n voting: %s \\n\" %(voting))\n",
    "    metric_consolidation(input_all, eclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  11   1  33]\n",
      " [  6  29   5  71]\n",
      " [  4  17   3  65]\n",
      " [  5  21   8 455]]\n",
      "accuracy Test Score: 66.80 +/- 0.00\n",
      "f1_weighted Test Score: 60.84 +/- 0.00\n",
      "roc_auc Test Score: 60.84 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# clf1 = svm.SVC(C=0.1,decision_function_shape='ovo', kernel='linear', \n",
    "#                max_iter=-1, random_state=100)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "clf3 = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                         max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
    "                                         min_samples_split=50, min_weight_fraction_leaf=0.0,\n",
    "                                         presort=False, random_state=100, splitter='best')\n",
    "clf4 = LogisticRegression(random_state=100, penalty = 'l1', C = 10**-1)\n",
    "clf5 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                         random_state=0)\n",
    "clf6 = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n",
    "                         n_estimators=100, random_state=100)\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('rf', clf2), ('dt', clf3) , ('lr', clf4), \n",
    "                                    ('bgt', clf5), ('bag', clf6)], voting='hard')\n",
    "\n",
    "metric_consolidation(input_all, eclf, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For XGBoost with: \n",
      " learning rate: 0.1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.42 +/- 2.18\n",
      "f1_weighted Test Score: 61.36 +/- 2.50\n",
      "roc_auc Test Score: 61.51 +/- 2.05\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 0.1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.98 +/- 2.31\n",
      "f1_weighted Test Score: 61.96 +/- 2.47\n",
      "roc_auc Test Score: 63.33 +/- 1.78\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 0.1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.89 +/- 1.90\n",
      "f1_weighted Test Score: 62.60 +/- 2.14\n",
      "roc_auc Test Score: 64.09 +/- 1.63\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 0.5 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.66 +/- 2.23\n",
      "f1_weighted Test Score: 63.67 +/- 2.32\n",
      "roc_auc Test Score: 65.54 +/- 1.91\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 0.5 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.18 +/- 2.11\n",
      "f1_weighted Test Score: 62.73 +/- 2.07\n",
      "roc_auc Test Score: 65.08 +/- 1.96\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 0.5 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.40 +/- 2.20\n",
      "f1_weighted Test Score: 63.18 +/- 2.23\n",
      "roc_auc Test Score: 65.37 +/- 2.24\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.51 +/- 2.27\n",
      "f1_weighted Test Score: 62.69 +/- 1.96\n",
      "roc_auc Test Score: 65.48 +/- 1.89\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.55 +/- 2.58\n",
      "f1_weighted Test Score: 62.88 +/- 2.03\n",
      "roc_auc Test Score: 65.74 +/- 1.51\n",
      "\n",
      "\n",
      "For XGBoost with: \n",
      " learning rate: 1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 65.19 +/- 2.51\n",
      "f1_weighted Test Score: 62.40 +/- 2.47\n",
      "roc_auc Test Score: 65.25 +/- 2.01\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Parameters\n",
    "learning_rate_values = [0.1, 0.5, 1]\n",
    "n_estimators_values = [50, 100, 150]\n",
    "\n",
    "for learning_rate in learning_rate_values:\n",
    "    for n_estimators in n_estimators_values:\n",
    "        clf = XGBClassifier(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "\n",
    "        print (\"For XGBoost with: \\n learning rate: %s \\n n_estimators: %s \\n\"\n",
    "              %(learning_rate, n_estimators))\n",
    "        \n",
    "        metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9  13   3  30]\n",
      " [ 11  29  12  59]\n",
      " [  5  21  12  51]\n",
      " [  6  26  17 440]]\n",
      "accuracy Test Score: 65.86 +/- 0.00\n",
      "f1_weighted Test Score: 62.14 +/- 0.00\n",
      "roc_auc Test Score: 63.77 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(learning_rate=0.5, n_estimators=50)\n",
    "\n",
    "metric_consolidation(input_all, clf, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
