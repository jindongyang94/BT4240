{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "\n",
    "1. Adaboost\n",
    "<br>\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e. models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions are then combined through a weighted majority vote (or sum) to product the final prediction. Used for classification and regression problems.\n",
    "<br><br>\n",
    "2. Gradient Tree Boosting\n",
    "<br>\n",
    "Generalizaiton of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "1. Random Forests\n",
    "<br>\n",
    "Each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split is chosen is no longer the best split among all the features. Instead the split that is picked is the best split among a random subset of the features.\n",
    "<br><br>\n",
    "2. Extremely Random Trees\n",
    "<br>\n",
    "Randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data before feature selection\n",
    "input_data_before_fs = pd.read_csv('processed_train.csv', index_col=0)\n",
    "\n",
    "# Input data after feature selection\n",
    "input_data_after_fs = pd.read_csv('processed_train_after_feature.csv', index_col=0)\n",
    "\n",
    "# List of all the input data\n",
    "input_all = {\n",
    "    \"normal_before_fs\" : input_data_before_fs,\n",
    "    \"normal_after_fs\" : input_data_after_fs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "def metric_consolidation(input_all, classifier, method = \"cross_validation\"):\n",
    "    metrics = {'accuracy': 'accuracy',\n",
    "               'roc_auc': make_scorer(multiclass_roc_auc_score, average='weighted'),\n",
    "               'f1_weighted': 'f1_weighted'\n",
    "              }\n",
    "    \n",
    "    for input_name, input_data in input_all.items():\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        # apply all metrics to the classifier for cross_validation\n",
    "        if method == \"cross_validation\":\n",
    "            scores = tenfold(classifier, x_train, y_train, metric = metrics)\n",
    "            print (\"Metrics for %s: \\n\" %input_name)\n",
    "            for metric in metrics:\n",
    "                test_score_name = \"test_\" + metric\n",
    "                test_score = scores[test_score_name]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100))   \n",
    "            print (\"\\n\")\n",
    "            \n",
    "        if method == \"test\":\n",
    "            y_pred = classifier.predict(x_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            roc_score = multiclass_roc_auc_score(y_test, y_pred, average='weighted')\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "            \n",
    "            metric_values = {'accuracy': accuracy,\n",
    "                             'roc_auc': roc_score,\n",
    "                             'f1_weighted': f1_weighted\n",
    "                            }\n",
    "            for metric in metrics:\n",
    "                test_score = metric_values[metric]\n",
    "                print (\"%s Test Score: %0.2f +/- %0.2f\" %(metric, test_score.mean()*100,\n",
    "                                               test_score.std()*100)) \n",
    "            \n",
    "def preprocessing(data):\n",
    "    #Split data into variables types - boolean, categorical, continuous, ID\n",
    "    bool_var = list(data.select_dtypes(['bool']))\n",
    "    cont_var = list(data.select_dtypes(['float64']))\n",
    "    cat_var = list(data.select_dtypes(['int64']))\n",
    "\n",
    "    #Input Data can be from all except id details\n",
    "    final_input_data = data[cat_var + cont_var + bool_var]\n",
    "    \n",
    "    x = final_input_data.loc[:, final_input_data.columns != 'Target'].values\n",
    "    y = final_input_data['Target'].values\n",
    "    y=y.astype('int')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, \n",
    "                                                    random_state = 100 , stratify = y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def tenfold(model, x, y, metric='accuracy'):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "    scores = cross_validate(model, x, y, cv=kfold, scoring=metric, \n",
    "                            return_train_score=True)\n",
    "    return scores\n",
    "\n",
    "# accuracy_mean = scores['test_score'].mean()\n",
    "# accuracy_std = scores['train_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Tree Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Random Trees\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging \n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = BaggingClassifier(LogisticRegression(), n_estimators=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "roc_auc Test Score: 55.38 +/- 1.61\n",
      "f1_weighted Test Score: 56.74 +/- 1.59\n",
      "accuracy Test Score: 67.34 +/- 1.10\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "roc_auc Test Score: 59.22 +/- 1.75\n",
      "f1_weighted Test Score: 59.34 +/- 1.69\n",
      "accuracy Test Score: 68.28 +/- 1.43\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "roc_auc Test Score: 60.19 +/- 1.70\n",
      "f1_weighted Test Score: 59.74 +/- 1.64\n",
      "accuracy Test Score: 68.10 +/- 1.49\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.5 \n",
      " n_estimators: 50 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9c0bba8bb479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             print (\"For Boosting with: \\n method: %s \\n learning rate: %s \\n n_estimators: %s \\n\"\n\u001b[0;32m     21\u001b[0m                   %(method, learning_rate, n_estimators))\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mmetric_consolidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-db38a4f95803>\u001b[0m in \u001b[0;36mmetric_consolidation\u001b[1;34m(input_all, classifier, method)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# apply all metrics to the classifier for cross_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cross_validation\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtenfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Metrics for %s: \\n\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-db38a4f95803>\u001b[0m in \u001b[0;36mtenfold\u001b[1;34m(model, x, y, metric)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     scores = cross_validate(model, x, y, cv=kfold, scoring=metric, \n\u001b[1;32m---> 75\u001b[1;33m                             return_train_score=True)\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    523\u001b[0m         estimator_weight = (-1. * self.learning_rate\n\u001b[0;32m    524\u001b[0m                             \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                             * (y_coding * np.log(y_predict_proba)).sum(axis=1))\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# Only boost the weights if it will fit again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Applications\\Anaconda\\envs\\neural\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m     34\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     35\u001b[0m          initial=_NoValue):\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Boosting Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "learning_rate_values = [0.1, 0.5, 1]\n",
    "n_estimators_values = [50, 100, 150]\n",
    "method_values = ['ada', 'gtb']\n",
    "\n",
    "for method in method_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for n_estimators in n_estimators_values:\n",
    "            \n",
    "            if method =='ada':\n",
    "                clf = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                         random_state=100)\n",
    "            elif method == 'gtb':\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                         random_state=100)\n",
    "            \n",
    "            print (\"For Boosting with: \\n method: %s \\n learning rate: %s \\n n_estimators: %s \\n\"\n",
    "                  %(method, learning_rate, n_estimators))\n",
    "            metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9   9   3  34]\n",
      " [  9  32   9  61]\n",
      " [  2  21   7  59]\n",
      " [  5  21  15 448]]\n",
      "f1_weighted Test Score: 61.93 +/- 0.00\n",
      "roc_auc Test Score: 62.55 +/- 0.00\n",
      "accuracy Test Score: 66.67 +/- 0.00\n",
      "[[  8  12   5  30]\n",
      " [  7  33  10  61]\n",
      " [  4  19   9  57]\n",
      " [  8  23  10 448]]\n",
      "f1_weighted Test Score: 62.44 +/- 0.00\n",
      "roc_auc Test Score: 63.43 +/- 0.00\n",
      "accuracy Test Score: 66.94 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Test Values for Boosting\n",
    "\n",
    "gtb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                         random_state=100)\n",
    "\n",
    "metric_consolidation(input_all, gtb, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 66.09 +/- 2.12\n",
      "roc_auc Test Score: 64.27 +/- 1.99\n",
      "f1_weighted Test Score: 62.39 +/- 2.60\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.24 +/- 1.39\n",
      "roc_auc Test Score: 62.27 +/- 1.78\n",
      "f1_weighted Test Score: 61.75 +/- 2.14\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.51 +/- 1.74\n",
      "roc_auc Test Score: 61.80 +/- 1.78\n",
      "f1_weighted Test Score: 61.63 +/- 2.57\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 63.84 +/- 2.12\n",
      "roc_auc Test Score: 62.13 +/- 2.09\n",
      "f1_weighted Test Score: 60.05 +/- 1.96\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.03 +/- 1.46\n",
      "roc_auc Test Score: 61.74 +/- 1.47\n",
      "f1_weighted Test Score: 61.09 +/- 1.67\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.70 +/- 1.79\n",
      "roc_auc Test Score: 61.00 +/- 1.31\n",
      "f1_weighted Test Score: 61.19 +/- 2.12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_estimators_values = [10, 50, 100] # Number of trees in the forest\n",
    "method_values = ['random', 'extreme']\n",
    "\n",
    "for method in method_values:\n",
    "    for n_estimators in n_estimators_values:\n",
    "\n",
    "        if method == 'random':\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, random_state=100)\n",
    "        elif method =='extreme':\n",
    "            clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=100)\n",
    "\n",
    "        print (\"For Boosting with: \\n method: %s \\n n_estimators: %s \\n\"\n",
    "              %(method, n_estimators))\n",
    "        metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc Test Score: 60.31 +/- 0.00\n",
      "f1_weighted Test Score: 60.79 +/- 0.00\n",
      "accuracy Test Score: 66.94 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "random = RandomForestClassifier(n_estimators=50, random_state=100)\n",
    "\n",
    "metric_consolidation(input_all, random, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Bagging with: \n",
      " method: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.38 +/- 1.49\n",
      "roc_auc Test Score: 57.93 +/- 1.87\n",
      "f1_weighted Test Score: 58.73 +/- 1.96\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.39 +/- 1.43\n",
      "roc_auc Test Score: 58.14 +/- 1.57\n",
      "f1_weighted Test Score: 58.87 +/- 1.96\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 67.43 +/- 1.35\n",
      "roc_auc Test Score: 58.12 +/- 1.47\n",
      "f1_weighted Test Score: 58.89 +/- 1.81\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 63.17 +/- 3.24\n",
      "roc_auc Test Score: 60.29 +/- 3.39\n",
      "f1_weighted Test Score: 58.86 +/- 3.39\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 63.40 +/- 2.42\n",
      "roc_auc Test Score: 59.74 +/- 2.31\n",
      "f1_weighted Test Score: 58.66 +/- 2.41\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 63.62 +/- 2.69\n",
      "roc_auc Test Score: 60.23 +/- 2.76\n",
      "f1_weighted Test Score: 59.03 +/- 2.70\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 64.39 +/- 2.19\n",
      "roc_auc Test Score: 64.86 +/- 2.10\n",
      "f1_weighted Test Score: 61.61 +/- 2.23\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.11 +/- 2.53\n",
      "roc_auc Test Score: 64.74 +/- 2.60\n",
      "f1_weighted Test Score: 63.05 +/- 3.28\n",
      "\n",
      "\n",
      "For Bagging with: \n",
      " method: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 68.24 +/- 3.17\n",
      "roc_auc Test Score: 64.40 +/- 2.93\n",
      "f1_weighted Test Score: 63.11 +/- 3.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bagging Parameters\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_estimators_values = [10, 50, 100] # Number of trees in the forest\n",
    "method_values = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
    "\n",
    "for method in method_values:\n",
    "    for n_estimators in n_estimators_values:\n",
    "\n",
    "        clf = BaggingClassifier(base_estimator=method, n_estimators=n_estimators, random_state=100)\n",
    "\n",
    "        print (\"For Bagging with: \\n method: %s \\n n_estimators: %s \\n\"\n",
    "              %(method, n_estimators))\n",
    "        metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc Test Score: 61.62 +/- 0.00\n",
      "f1_weighted Test Score: 61.73 +/- 0.00\n",
      "accuracy Test Score: 66.13 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=100)\n",
    "\n",
    "metric_consolidation(input_all, bagging, method='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize GTB\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "gtb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                         random_state=100)\n",
    "\n",
    "# split the data\n",
    "x_train, x_test, y_train, y_test = preprocessing(input_data_after_fs)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "gtb.fit(x_train, y_train)\n",
    "\n",
    "# Get the tree number 50\n",
    "sub_tree_50 = gtb.estimators_[50, 0]\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    sub_tree_50,\n",
    "    out_file=None, filled=True,\n",
    "    rounded=True,  \n",
    "    special_characters=True,\n",
    "    proportion=True,\n",
    ")\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) \n",
    "graph.write_pdf(\"dt_boosting_50.pdf\")\n",
    "graph.write_png(\"dt_boosting_50.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
