{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "\n",
    "1. Adaboost\n",
    "<br>\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e. models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions are then combined through a weighted majority vote (or sum) to product the final prediction. Used for classification and regression problems.\n",
    "<br><br>\n",
    "2. Gradient Tree Boosting\n",
    "<br>\n",
    "Generalizaiton of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "1. Random Forests\n",
    "<br>\n",
    "Each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split is chosen is no longer the best split among all the features. Instead the split that is picked is the best split among a random subset of the features.\n",
    "<br><br>\n",
    "2. Extremely Random Trees\n",
    "<br>\n",
    "Randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data before feature selection\n",
    "input_data_before_fs = pd.read_csv('processed_train.csv', index_col=0)\n",
    "\n",
    "# Input data after feature selection\n",
    "input_data_after_fs = pd.read_csv('processed_train_after_feature.csv', index_col=0)\n",
    "\n",
    "# Upsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "# Downsampling without feature selection\n",
    "\n",
    "# Upsampling with feature selection\n",
    "\n",
    "\n",
    "# List of all the input data\n",
    "input_all = {\n",
    "    \"normal_before_fs\" : input_data_before_fs,\n",
    "#     \"normal_after_fs\" : input_data_after_fs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "def metric_consolidation(input_all, classifier, method = \"cross_validation\"):\n",
    "    metrics = {'accuracy': 'accuracy',\n",
    "               'f1_weighted': 'f1_weighted',\n",
    "               'roc_auc': make_scorer(multiclass_roc_auc_score, average='weighted')\n",
    "              }\n",
    "    \n",
    "    for input_name, input_data in input_all.items():\n",
    "        # split the data\n",
    "        x_train, x_test, y_train, y_test = preprocessing(input_data)\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        # apply all metrics to the classifier for cross_validation\n",
    "        if method == \"cross_validation\":\n",
    "            scores = tenfold(classifier, x_train, y_train, metric = metrics)\n",
    "            print (\"Metrics for %s: \\n\" %input_name)\n",
    "            for metric in metrics:\n",
    "                test_score_name = \"test_\" + metric\n",
    "                test_score = scores[test_score_name]\n",
    "                print (\"%s Test Score: %f +/- %f\" %(metric, test_score.mean(),\n",
    "                                               test_score.std()))   \n",
    "            print (\"\\n\")\n",
    "            \n",
    "def preprocessing(data):\n",
    "    #Split data into variables types - boolean, categorical, continuous, ID\n",
    "    bool_var = list(data.select_dtypes(['bool']))\n",
    "    cont_var = list(data.select_dtypes(['float64']))\n",
    "    cat_var = list(data.select_dtypes(['int64']))\n",
    "\n",
    "    #Input Data can be from all except id details\n",
    "    final_input_data = data[cat_var + cont_var + bool_var]\n",
    "    \n",
    "    x = final_input_data.loc[:, final_input_data.columns != 'Target'].values\n",
    "    y = final_input_data['Target'].values\n",
    "    y=y.astype('int')\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, \n",
    "                                                    random_state = 100 , stratify = y)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def tenfold(model, x, y, metric='accuracy'):\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=100, shuffle=True)\n",
    "    scores = cross_validate(model, x, y, cv=kfold, scoring=metric, \n",
    "                            return_train_score=True)\n",
    "    return scores\n",
    "\n",
    "# accuracy_mean = scores['test_score'].mean()\n",
    "# accuracy_std = scores['train_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Tree Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                 max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme Random Trees\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.673432 +/- 0.010978\n",
      "f1_weighted Test Score: 0.567380 +/- 0.015882\n",
      "roc_auc Test Score: 0.553819 +/- 0.016133\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.682820 +/- 0.014255\n",
      "f1_weighted Test Score: 0.593405 +/- 0.016860\n",
      "roc_auc Test Score: 0.592154 +/- 0.017451\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.681036 +/- 0.014907\n",
      "f1_weighted Test Score: 0.597380 +/- 0.016394\n",
      "roc_auc Test Score: 0.601856 +/- 0.016990\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.5 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.674774 +/- 0.014374\n",
      "f1_weighted Test Score: 0.607183 +/- 0.018601\n",
      "roc_auc Test Score: 0.617471 +/- 0.016075\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.5 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.673016 +/- 0.019268\n",
      "f1_weighted Test Score: 0.620297 +/- 0.020162\n",
      "roc_auc Test Score: 0.634851 +/- 0.014301\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 0.5 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.673423 +/- 0.018802\n",
      "f1_weighted Test Score: 0.629004 +/- 0.019580\n",
      "roc_auc Test Score: 0.648321 +/- 0.015191\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.664928 +/- 0.018340\n",
      "f1_weighted Test Score: 0.620061 +/- 0.017520\n",
      "roc_auc Test Score: 0.639175 +/- 0.014680\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.655521 +/- 0.023329\n",
      "f1_weighted Test Score: 0.626039 +/- 0.024363\n",
      "roc_auc Test Score: 0.653351 +/- 0.021663\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: ada \n",
      " learning rate: 1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.632197 +/- 0.032746\n",
      "f1_weighted Test Score: 0.611397 +/- 0.028129\n",
      "roc_auc Test Score: 0.650359 +/- 0.030847\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.684179 +/- 0.020762\n",
      "f1_weighted Test Score: 0.624977 +/- 0.023268\n",
      "roc_auc Test Score: 0.629715 +/- 0.021025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.681523 +/- 0.022094\n",
      "f1_weighted Test Score: 0.633532 +/- 0.022644\n",
      "roc_auc Test Score: 0.648658 +/- 0.017759\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.673931 +/- 0.023774\n",
      "f1_weighted Test Score: 0.630452 +/- 0.022587\n",
      "roc_auc Test Score: 0.649230 +/- 0.018181\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.5 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.652830 +/- 0.022152\n",
      "f1_weighted Test Score: 0.624686 +/- 0.018984\n",
      "roc_auc Test Score: 0.648627 +/- 0.021238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.5 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.654638 +/- 0.021648\n",
      "f1_weighted Test Score: 0.628088 +/- 0.019693\n",
      "roc_auc Test Score: 0.655464 +/- 0.021257\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 0.5 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.646532 +/- 0.023023\n",
      "f1_weighted Test Score: 0.621645 +/- 0.022357\n",
      "roc_auc Test Score: 0.649795 +/- 0.020733\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 1 \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.645608 +/- 0.027771\n",
      "f1_weighted Test Score: 0.627286 +/- 0.023120\n",
      "roc_auc Test Score: 0.657432 +/- 0.022543\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 1 \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.648293 +/- 0.025926\n",
      "f1_weighted Test Score: 0.628752 +/- 0.020128\n",
      "roc_auc Test Score: 0.658501 +/- 0.017561\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: gtb \n",
      " learning rate: 1 \n",
      " n_estimators: 150 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.652335 +/- 0.013413\n",
      "f1_weighted Test Score: 0.629724 +/- 0.008857\n",
      "roc_auc Test Score: 0.658157 +/- 0.010383\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "learning_rate_values = [0.1, 0.5, 1]\n",
    "n_estimators_values = [50, 100, 150]\n",
    "method_values = ['ada', 'gtb']\n",
    "\n",
    "for method in method_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for n_estimators in n_estimators_values:\n",
    "            \n",
    "            if method =='ada':\n",
    "                clf = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                         random_state=0)\n",
    "            elif method == 'gtb':\n",
    "                clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                         random_state=0)\n",
    "            \n",
    "            print (\"For Boosting with: \\n method: %s \\n learning rate: %s \\n n_estimators: %s \\n\"\n",
    "                  %(method, learning_rate, n_estimators))\n",
    "            metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.660936 +/- 0.021159\n",
      "f1_weighted Test Score: 0.623904 +/- 0.026006\n",
      "roc_auc Test Score: 0.642747 +/- 0.019939\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.682365 +/- 0.013944\n",
      "f1_weighted Test Score: 0.617516 +/- 0.021433\n",
      "roc_auc Test Score: 0.622716 +/- 0.017814\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: random \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.685118 +/- 0.017376\n",
      "f1_weighted Test Score: 0.616281 +/- 0.025718\n",
      "roc_auc Test Score: 0.617975 +/- 0.017786\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 10 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.638449 +/- 0.021202\n",
      "f1_weighted Test Score: 0.600485 +/- 0.019625\n",
      "roc_auc Test Score: 0.621277 +/- 0.020917\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 50 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.670255 +/- 0.014585\n",
      "f1_weighted Test Score: 0.610939 +/- 0.016665\n",
      "roc_auc Test Score: 0.617381 +/- 0.014718\n",
      "\n",
      "\n",
      "For Boosting with: \n",
      " method: extreme \n",
      " n_estimators: 100 \n",
      "\n",
      "Metrics for normal_before_fs: \n",
      "\n",
      "accuracy Test Score: 0.677044 +/- 0.017856\n",
      "f1_weighted Test Score: 0.611863 +/- 0.021210\n",
      "roc_auc Test Score: 0.609982 +/- 0.013119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Parameters\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_estimators_values = [10, 50, 100] # Number of trees in the forest\n",
    "method_values = ['random', 'extreme']\n",
    "\n",
    "for method in method_values:\n",
    "    for n_estimators in n_estimators_values:\n",
    "\n",
    "        if method == 'random':\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "        elif method =='extreme':\n",
    "            clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=0)\n",
    "\n",
    "        print (\"For Boosting with: \\n method: %s \\n n_estimators: %s \\n\"\n",
    "              %(method, n_estimators))\n",
    "        metric_consolidation(input_all, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
